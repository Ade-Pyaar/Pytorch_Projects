{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN - Character Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3_rN3gsNicC"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9J8VsJmOKb1"
      },
      "source": [
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL3tPdCFOgh7",
        "outputId": "38782cac-28c0-46a5-f87b-5fb77c14b9f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z4otaMMO4Mr"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4RKL1xHO6bW"
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVoyDHT7Peo9",
        "outputId": "c58b38d6-3077-4659-95a9-dbf7018c6334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 70,  2, 30,  7, 76,  0, 67, 37, 21, 21, 21, 19,  2, 30, 30, 57,\n",
              "       67,  5,  2, 16, 69, 64, 69, 76, 51, 67,  2,  0, 76, 67,  2, 64, 64,\n",
              "       67,  2, 64, 69, 31, 76, 65, 67, 76, 62, 76,  0, 57, 67, 75, 72, 70,\n",
              "        2, 30, 30, 57, 67,  5,  2, 16, 69, 64, 57, 67, 69, 51, 67, 75, 72,\n",
              "       70,  2, 30, 30, 57, 67, 69, 72, 67, 69,  7, 51, 67, 10, 36, 72, 21,\n",
              "       36,  2, 57, 74, 21, 21, 50, 62, 76,  0, 57,  7, 70, 69, 72])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuxmI5yqPxNR"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWoeZpRyQkUs"
      },
      "source": [
        "# Pre-process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNKaIVtWQnha"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avWHNJ4kRHsW",
        "outputId": "7a825434-1428-42df-9555-5312ff949d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check the function works well\n",
        "\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwDIBLjEReIv"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vF6CaUdSkkG"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S89pJSRIUu8w"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEGLIC44U20S",
        "outputId": "aa1ccf9f-b2b2-49ba-d5fc-4c9b1d21abc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('x\\n', x[:10, :10])\n",
        "print('y\\n', y[:10, :10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 1 70  2 30  7 76  0 67 37 21]\n",
            " [51 10 72 67  7 70  2  7 67  2]\n",
            " [76 72 12 67 10  0 67  2 67  5]\n",
            " [51 67  7 70 76 67 78 70 69 76]\n",
            " [67 51  2 36 67 70 76  0 67  7]\n",
            " [78 75 51 51 69 10 72 67  2 72]\n",
            " [67 22 72 72  2 67 70  2 12 67]\n",
            " [17 42 64 10 72 51 31 57 74 67]]\n",
            "y\n",
            " [[70  2 30  7 76  0 67 37 21 21]\n",
            " [10 72 67  7 70  2  7 67  2  7]\n",
            " [72 12 67 10  0 67  2 67  5 10]\n",
            " [67  7 70 76 67 78 70 69 76  5]\n",
            " [51  2 36 67 70 76  0 67  7 76]\n",
            " [75 51 51 69 10 72 67  2 72 12]\n",
            " [22 72 72  2 67 70  2 12 67 51]\n",
            " [42 64 10 72 51 31 57 74 67 32]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkfLSNuwVCab"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfrj9b2HViAx",
        "outputId": "2b483477-7bfa-4d91-c1cb-3c067e1a22ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if (train_on_gpu):\n",
        "    print('Training on GPU')\n",
        "else:\n",
        "    print('Training on CPU')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEceQt9rVyaV"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO89WhaJWHP5"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0sQ9XWtZz8x"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWJfWze5Z0-o"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr89Q-fQZ3C4"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    \n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8H_Vu4ieSQI",
        "outputId": "413c5f46-6eba-41ef-a85e-a9cd57cc1309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8riiFHVebpb"
      },
      "source": [
        "\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLgPLLd_fKFL",
        "outputId": "45f16110-579d-454a-a70f-79f9ab6188b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2609... Val Loss: 3.2012\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1542... Val Loss: 3.1350\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1447... Val Loss: 3.1233\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1158... Val Loss: 3.1196\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1432... Val Loss: 3.1175\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1206... Val Loss: 3.1155\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1111... Val Loss: 3.1133\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1174... Val Loss: 3.1070\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1073... Val Loss: 3.0915\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0685... Val Loss: 3.0537\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0101... Val Loss: 2.9868\n",
            "Epoch: 1/20... Step: 120... Loss: 2.8713... Val Loss: 2.8525\n",
            "Epoch: 1/20... Step: 130... Loss: 2.8071... Val Loss: 2.7745\n",
            "Epoch: 2/20... Step: 140... Loss: 2.6802... Val Loss: 2.6289\n",
            "Epoch: 2/20... Step: 150... Loss: 2.6046... Val Loss: 2.5412\n",
            "Epoch: 2/20... Step: 160... Loss: 2.5342... Val Loss: 2.4892\n",
            "Epoch: 2/20... Step: 170... Loss: 2.4568... Val Loss: 2.4413\n",
            "Epoch: 2/20... Step: 180... Loss: 2.4296... Val Loss: 2.4011\n",
            "Epoch: 2/20... Step: 190... Loss: 2.3833... Val Loss: 2.3726\n",
            "Epoch: 2/20... Step: 200... Loss: 2.3676... Val Loss: 2.3855\n",
            "Epoch: 2/20... Step: 210... Loss: 2.3444... Val Loss: 2.3246\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3036... Val Loss: 2.2880\n",
            "Epoch: 2/20... Step: 230... Loss: 2.2837... Val Loss: 2.2576\n",
            "Epoch: 2/20... Step: 240... Loss: 2.2695... Val Loss: 2.2357\n",
            "Epoch: 2/20... Step: 250... Loss: 2.1982... Val Loss: 2.2051\n",
            "Epoch: 2/20... Step: 260... Loss: 2.1776... Val Loss: 2.1753\n",
            "Epoch: 2/20... Step: 270... Loss: 2.1828... Val Loss: 2.1537\n",
            "Epoch: 3/20... Step: 280... Loss: 2.1767... Val Loss: 2.1310\n",
            "Epoch: 3/20... Step: 290... Loss: 2.1435... Val Loss: 2.1030\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1147... Val Loss: 2.0842\n",
            "Epoch: 3/20... Step: 310... Loss: 2.0950... Val Loss: 2.0651\n",
            "Epoch: 3/20... Step: 320... Loss: 2.0586... Val Loss: 2.0442\n",
            "Epoch: 3/20... Step: 330... Loss: 2.0402... Val Loss: 2.0368\n",
            "Epoch: 3/20... Step: 340... Loss: 2.0577... Val Loss: 2.0117\n",
            "Epoch: 3/20... Step: 350... Loss: 2.0421... Val Loss: 1.9962\n",
            "Epoch: 3/20... Step: 360... Loss: 1.9644... Val Loss: 1.9774\n",
            "Epoch: 3/20... Step: 370... Loss: 1.9996... Val Loss: 1.9611\n",
            "Epoch: 3/20... Step: 380... Loss: 1.9659... Val Loss: 1.9436\n",
            "Epoch: 3/20... Step: 390... Loss: 1.9490... Val Loss: 1.9294\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9079... Val Loss: 1.9138\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9296... Val Loss: 1.8958\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9095... Val Loss: 1.8872\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9070... Val Loss: 1.8694\n",
            "Epoch: 4/20... Step: 440... Loss: 1.8870... Val Loss: 1.8568\n",
            "Epoch: 4/20... Step: 450... Loss: 1.8332... Val Loss: 1.8428\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8208... Val Loss: 1.8341\n",
            "Epoch: 4/20... Step: 470... Loss: 1.8508... Val Loss: 1.8238\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8361... Val Loss: 1.8122\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8374... Val Loss: 1.8024\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8249... Val Loss: 1.7917\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8051... Val Loss: 1.7804\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8286... Val Loss: 1.7706\n",
            "Epoch: 4/20... Step: 530... Loss: 1.7810... Val Loss: 1.7625\n",
            "Epoch: 4/20... Step: 540... Loss: 1.7503... Val Loss: 1.7508\n",
            "Epoch: 4/20... Step: 550... Loss: 1.7847... Val Loss: 1.7387\n",
            "Epoch: 5/20... Step: 560... Loss: 1.7497... Val Loss: 1.7284\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7488... Val Loss: 1.7214\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7221... Val Loss: 1.7140\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7240... Val Loss: 1.7048\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7167... Val Loss: 1.6990\n",
            "Epoch: 5/20... Step: 610... Loss: 1.6943... Val Loss: 1.6896\n",
            "Epoch: 5/20... Step: 620... Loss: 1.6969... Val Loss: 1.6854\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7123... Val Loss: 1.6756\n",
            "Epoch: 5/20... Step: 640... Loss: 1.6812... Val Loss: 1.6689\n",
            "Epoch: 5/20... Step: 650... Loss: 1.6659... Val Loss: 1.6618\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6536... Val Loss: 1.6568\n",
            "Epoch: 5/20... Step: 670... Loss: 1.6812... Val Loss: 1.6503\n",
            "Epoch: 5/20... Step: 680... Loss: 1.6652... Val Loss: 1.6434\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6486... Val Loss: 1.6349\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6425... Val Loss: 1.6285\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6480... Val Loss: 1.6289\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6169... Val Loss: 1.6166\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6406... Val Loss: 1.6073\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6054... Val Loss: 1.6097\n",
            "Epoch: 6/20... Step: 750... Loss: 1.5817... Val Loss: 1.6027\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6307... Val Loss: 1.5943\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6079... Val Loss: 1.5929\n",
            "Epoch: 6/20... Step: 780... Loss: 1.5922... Val Loss: 1.5898\n",
            "Epoch: 6/20... Step: 790... Loss: 1.5753... Val Loss: 1.5813\n",
            "Epoch: 6/20... Step: 800... Loss: 1.5882... Val Loss: 1.5789\n",
            "Epoch: 6/20... Step: 810... Loss: 1.5847... Val Loss: 1.5756\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5458... Val Loss: 1.5674\n",
            "Epoch: 6/20... Step: 830... Loss: 1.5792... Val Loss: 1.5613\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5362... Val Loss: 1.5562\n",
            "Epoch: 7/20... Step: 850... Loss: 1.5612... Val Loss: 1.5549\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5415... Val Loss: 1.5503\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5555... Val Loss: 1.5430\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5482... Val Loss: 1.5425\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5481... Val Loss: 1.5379\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5367... Val Loss: 1.5308\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5031... Val Loss: 1.5314\n",
            "Epoch: 7/20... Step: 920... Loss: 1.5304... Val Loss: 1.5287\n",
            "Epoch: 7/20... Step: 930... Loss: 1.5117... Val Loss: 1.5223\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5235... Val Loss: 1.5203\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5264... Val Loss: 1.5166\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5248... Val Loss: 1.5110\n",
            "Epoch: 7/20... Step: 970... Loss: 1.5340... Val Loss: 1.5073\n",
            "Epoch: 8/20... Step: 980... Loss: 1.4968... Val Loss: 1.5072\n",
            "Epoch: 8/20... Step: 990... Loss: 1.5071... Val Loss: 1.4988\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.5020... Val Loss: 1.4973\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5330... Val Loss: 1.4973\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5017... Val Loss: 1.4941\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.4869... Val Loss: 1.4876\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.4979... Val Loss: 1.4884\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.4750... Val Loss: 1.4871\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.4736... Val Loss: 1.4805\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.4823... Val Loss: 1.4777\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.4829... Val Loss: 1.4752\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.4522... Val Loss: 1.4734\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.4582... Val Loss: 1.4701\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.4539... Val Loss: 1.4658\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.4682... Val Loss: 1.4678\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.4612... Val Loss: 1.4625\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.4620... Val Loss: 1.4546\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.4824... Val Loss: 1.4589\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4355... Val Loss: 1.4545\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4452... Val Loss: 1.4507\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.4273... Val Loss: 1.4521\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.4677... Val Loss: 1.4484\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4215... Val Loss: 1.4481\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.4286... Val Loss: 1.4422\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4319... Val Loss: 1.4408\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4103... Val Loss: 1.4399\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4127... Val Loss: 1.4390\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.4300... Val Loss: 1.4306\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4335... Val Loss: 1.4333\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4257... Val Loss: 1.4294\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4364... Val Loss: 1.4257\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4252... Val Loss: 1.4279\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4106... Val Loss: 1.4247\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4117... Val Loss: 1.4210\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.3877... Val Loss: 1.4203\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.3975... Val Loss: 1.4214\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.3776... Val Loss: 1.4190\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.3751... Val Loss: 1.4145\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.3862... Val Loss: 1.4135\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.3715... Val Loss: 1.4117\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4100... Val Loss: 1.4105\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4184... Val Loss: 1.4086\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4168... Val Loss: 1.4082\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4331... Val Loss: 1.4060\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4226... Val Loss: 1.4012\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.3788... Val Loss: 1.4058\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4188... Val Loss: 1.4013\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.3473... Val Loss: 1.4017\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.3660... Val Loss: 1.3989\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3625... Val Loss: 1.3963\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.3802... Val Loss: 1.3901\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.3728... Val Loss: 1.3895\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.3509... Val Loss: 1.3922\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.3312... Val Loss: 1.3932\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.3828... Val Loss: 1.3855\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4280... Val Loss: 1.3838\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.3789... Val Loss: 1.3810\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.3786... Val Loss: 1.3784\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.3940... Val Loss: 1.3770\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.3413... Val Loss: 1.3810\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3240... Val Loss: 1.3767\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3196... Val Loss: 1.3819\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.3552... Val Loss: 1.3763\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3332... Val Loss: 1.3743\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3336... Val Loss: 1.3702\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.3553... Val Loss: 1.3655\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3418... Val Loss: 1.3698\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.2990... Val Loss: 1.3675\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.3641... Val Loss: 1.3622\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.3320... Val Loss: 1.3626\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.3463... Val Loss: 1.3571\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3252... Val Loss: 1.3535\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3263... Val Loss: 1.3569\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.2984... Val Loss: 1.3564\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3060... Val Loss: 1.3531\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.3547... Val Loss: 1.3494\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3103... Val Loss: 1.3486\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.2832... Val Loss: 1.3513\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3123... Val Loss: 1.3447\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3378... Val Loss: 1.3458\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3071... Val Loss: 1.3423\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.2840... Val Loss: 1.3432\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3225... Val Loss: 1.3383\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3092... Val Loss: 1.3489\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3030... Val Loss: 1.3387\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3252... Val Loss: 1.3307\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.2674... Val Loss: 1.3390\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.2476... Val Loss: 1.3359\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3068... Val Loss: 1.3326\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3150... Val Loss: 1.3336\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3088... Val Loss: 1.3339\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3140... Val Loss: 1.3293\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.2958... Val Loss: 1.3265\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3058... Val Loss: 1.3229\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.2931... Val Loss: 1.3261\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.2516... Val Loss: 1.3302\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3169... Val Loss: 1.3269\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.2898... Val Loss: 1.3247\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.2827... Val Loss: 1.3191\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.2851... Val Loss: 1.3151\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.2697... Val Loss: 1.3260\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.2667... Val Loss: 1.3192\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.2596... Val Loss: 1.3129\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.2651... Val Loss: 1.3117\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.2873... Val Loss: 1.3214\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.2635... Val Loss: 1.3110\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.2669... Val Loss: 1.3113\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.2530... Val Loss: 1.3165\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.2650... Val Loss: 1.3111\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.2741... Val Loss: 1.3100\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.2686... Val Loss: 1.3062\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.2834... Val Loss: 1.3121\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.2653... Val Loss: 1.3057\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.2565... Val Loss: 1.3014\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.2661... Val Loss: 1.3091\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.2425... Val Loss: 1.3098\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.2523... Val Loss: 1.3048\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.2743... Val Loss: 1.3037\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2565... Val Loss: 1.3087\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.2514... Val Loss: 1.3028\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.2494... Val Loss: 1.3076\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.2681... Val Loss: 1.2998\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.2454... Val Loss: 1.3052\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2108... Val Loss: 1.3031\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.2593... Val Loss: 1.2985\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2306... Val Loss: 1.3022\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2450... Val Loss: 1.2978\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2222... Val Loss: 1.3030\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.2397... Val Loss: 1.3022\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2460... Val Loss: 1.2963\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2512... Val Loss: 1.2944\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.2503... Val Loss: 1.2933\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2175... Val Loss: 1.2934\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.2382... Val Loss: 1.2982\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2268... Val Loss: 1.2937\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2232... Val Loss: 1.2959\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.2585... Val Loss: 1.2929\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.2503... Val Loss: 1.2909\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.2634... Val Loss: 1.3010\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2184... Val Loss: 1.2944\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2331... Val Loss: 1.2856\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2294... Val Loss: 1.2867\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.2468... Val Loss: 1.2989\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2498... Val Loss: 1.2833\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2257... Val Loss: 1.2883\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2318... Val Loss: 1.2862\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2233... Val Loss: 1.2847\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2175... Val Loss: 1.2878\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2325... Val Loss: 1.2871\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2289... Val Loss: 1.2856\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2174... Val Loss: 1.2891\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2136... Val Loss: 1.2814\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2164... Val Loss: 1.2879\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2246... Val Loss: 1.2819\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2274... Val Loss: 1.2779\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2353... Val Loss: 1.2779\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2459... Val Loss: 1.2794\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2097... Val Loss: 1.2790\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2157... Val Loss: 1.2781\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2073... Val Loss: 1.2793\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2429... Val Loss: 1.2771\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2069... Val Loss: 1.2794\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.1959... Val Loss: 1.2771\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2177... Val Loss: 1.2840\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.1946... Val Loss: 1.2797\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.1956... Val Loss: 1.2770\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2199... Val Loss: 1.2808\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2214... Val Loss: 1.2795\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2142... Val Loss: 1.2718\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2258... Val Loss: 1.2712\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2167... Val Loss: 1.2757\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2073... Val Loss: 1.2741\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2154... Val Loss: 1.2684\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.1814... Val Loss: 1.2749\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.1879... Val Loss: 1.2743\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.1790... Val Loss: 1.2698\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.1867... Val Loss: 1.2687\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.1856... Val Loss: 1.2787\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.1786... Val Loss: 1.2715\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2247... Val Loss: 1.2685\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2383... Val Loss: 1.2740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hng81GOabPbc"
      },
      "source": [
        "# Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZvWnX5GbRk-"
      },
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnRwu7rIbppY"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJrCGHxXbq9a"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()\n",
        "\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "    return net.int2char[char], h"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoxIOMzpdOWN"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AexVCJeeB6R",
        "outputId": "f31a99b7-36af-4c8a-8bf5-51d121624a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna\n",
            "Alexandrovna's health of all she was simply to him. He saw\n",
            "at that tears with the possible weading, and whoseled with the\n",
            "precious time to the chairs in his house.\n",
            "\n",
            "\"I had all more all more because they make him to think about have\n",
            "it at once to speak to her,\" he said, at the state of his\n",
            "study, and the day work of the country had said his head and struggle\n",
            "of her.\n",
            "\n",
            "\"What is to be the carringer?\" thought Levin, sighed to his\n",
            "hands, he had seriously asking a bread. Tringing to this\n",
            "trees. A signisication had bare arminated, and he were\n",
            "all sides and transears, but as they had to do the sould of\n",
            "this are complete chueling, and he had no stayed of his head and\n",
            "considered this shaming, and so as his candle and he foonmed the profised of his\n",
            "world, but the straish on stending over weads and her face.\n",
            "\n",
            "\"I'll see him!\"\n",
            "\n",
            "\"No doubt it can be interested,\" said Anna, at the point of the\n",
            "corridor, and he heard with his walk of a chalk to the ball\n",
            "wait and conceaded angry, and smiling, stord and an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_KQKQguedhM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVTvGCd8e1K-"
      },
      "source": [
        "# Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xKNqevre3b4",
        "outputId": "046469dc-0ea3-4709-9d5e-fbad6b3a971f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw8mo43_fAa2",
        "outputId": "0b077b9e-176d-4417-9820-b0d31c54826d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(sample(loaded, 2000, top_k=5, prime=\"And Nike said\"))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Nike said he\n",
            "had now serious to him all the point. In the works was so much a man\n",
            "walked out of the same start.\n",
            "\n",
            "After a smell of an end of a consideration of sense of the cracks. The\n",
            "cricking hat had not been that the whole means was so fal at the\n",
            "point of anything to a sign. And had been as soon and to his side\n",
            "of since he was so delightful, that she had been so life, his mistres,\n",
            "tho good nature and all hereess and herestly and husband and simple of\n",
            "attitude. She had an idea would not breath the same\n",
            "at the same time in the station of the perform, and he went up\n",
            "herself at the signess and at once that she was so good to all\n",
            "the person he had been saying, he had the sick man, had that it was\n",
            "never seen. She stood at him.\n",
            "\n",
            "\"Yes, you can't be in acquaintance too. I'm all to help the streagon\n",
            "of the mare. This some ofting me, and see her in the stairs,\n",
            "it could not conscious of his sun had setted herself and step and\n",
            "seeing anything, that you will strange his sense of most forest to ter\n",
            "the memories or shame, and that's all those time.\"\n",
            "\n",
            "\"Yes, I can't believe anything in horses, and I have so strange that\n",
            "there was no more candles, and he changed him. That's a man to answer\n",
            "the position.\"\n",
            "\n",
            "\"Yes, you would have been all means. Well, I am not to go away. We was\n",
            "all was already such a ceasent. Well, and there is it.\"\n",
            "\n",
            "\"Yes, I don't know that I was indeed at many and say. I don't know\n",
            "it, and would be doing them in an early about took\" he said,\n",
            "still all any sort of servants when he had sorn and her husband\n",
            "was in such place it was that that were a passing that when\n",
            "she was stirred, how he had been daughter, and all that there\n",
            "were stop him with him, and so it was not all there. He was\n",
            "the position of the chair, that time a sense the conversation would\n",
            "be the sensation of several statess, and he finished her, he called\n",
            "the peasants, and was so not to see him for a boy on their party, and\n",
            "her sisters. He had been delightful as she was said in a\n",
            "presticity for her as she was at once\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lts34NnqfI9-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}